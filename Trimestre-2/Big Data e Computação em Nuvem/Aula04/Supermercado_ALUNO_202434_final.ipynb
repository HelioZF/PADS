{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOk2BLo9v7pP",
    "tags": []
   },
   "source": [
    "# Insper\n",
    "## Programa Avançado em Data Science e Decisão\n",
    "## Big Data e Computação em Nuvem\n",
    "\n",
    "Prof. Michel Fornaciali\n",
    "\n",
    "Prof. Thanuci Silva\n",
    "\n",
    "\n",
    "**Do que praticamos na aula passada para o que vamos praticar hoje:**\n",
    "\n",
    "Tabela comparativa entre a programação utilizando Dask e a programação usando Spark RDDs.\n",
    "\n",
    "| Característica        | Dask                                   | Apache Spark RDDs                    |\n",
    "|-----------------------|----------------------------------------|--------------------------------------|\n",
    "| **Modelo de Programação** | APIs paralelas para arrays, dataframes e listas. Agendamento dinâmico e execução em tempo real. | Baseado em RDDs, coleções distribuídas de objetos imutáveis. Opera em lotes. |\n",
    "| **Escala e Performance** | Escala de uma máquina a clusters; útil mesmo em hardware limitado. Bom para dados além da memória. | Ideal para grandes volumes de dados em clusters. Alta eficiência com processamento em memória. |\n",
    "| **Facilidade de Uso** | Fácil integração com NumPy, pandas e scikit-learn. Favorável para usuários desses pacotes. | Curva de aprendizado mais íngreme. Integração com SQL, machine learning, processamento de grafos e streaming. |\n",
    "| **Comunidade e Suporte** | Comunidade menor, **mas crescente**. Popular na comunidade de ciência de dados Python. | Amplo suporte de uma grande comunidade e uso industrial. Suporte comercial extenso. |\n",
    "| **Caso de Uso** | Análises interativas em larga escala, integração com ferramentas de análise de dados Python. | Processamento de dados em grande escala, transformações complexas, alta tolerância a falhas. |\n",
    "\n",
    "\n",
    "\n",
    "# Prática - Supermercado \n",
    "\n",
    "Faça códigos usando Spark para responder as perguntas abaixo: \n",
    "\n",
    "- **Qual o valor total da compra?** \n",
    "- **Quantos produtos foram comprados? Se representar uma fração, considere um único produto**\n",
    "- **Qual o produto mais caro (em termos de valor unitário)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "m9uVzmjjnbYG"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "                    .builder \\\n",
    "                    .master('local[2]') \\\n",
    "                    .appName('supermercado_Thanuci') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "SfnXWkWOnkR3",
    "outputId": "76649696-dae0-4b45-9cfc-83e70e435038"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-thanuci-20silva:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>supermercado_Thanuci</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe9a038f490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "V0xcamhSux3t"
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2AnXHlcWuqbq"
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.textFile(\"supermercado.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supermercado.csv MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "rdd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XR2O_huYxQXQ",
    "outputId": "99cd0ac9-0c3d-4cb9-c177-304eb4b8f3b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PRODUTO;QUANTIDADE;PRECO UNIT. (R$)', 'Achocolatado;2;5.89', 'Acucar;3;2.03']\n"
     ]
    }
   ],
   "source": [
    "print(rdd1.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Dv8hOWPbEuk"
   },
   "source": [
    "## Tarefa 1: Organizando o RDD\n",
    "\n",
    "* [pyspark.RDD.map](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html)\n",
    "\n",
    "   Return a new RDD by applying a function **to each element** of this RDD.\n",
    "\n",
    "\n",
    "* [pyspark.RDD.filter](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.filter.html)\n",
    "\n",
    "  Return a new RDD containing **only the elements** that satisfy a predicate.\n",
    "  \n",
    "* [string.split()](https://stackoverflow.com/questions/40955656/how-does-python-split-function-works)\n",
    "\n",
    "    Will break and **split the string on the argument that is passed** and **return all the parts in a list**. The list will not include the splitting character(s).\n",
    "\n",
    "Dito isso:\n",
    "\n",
    "1. **Remova o cabeçalho**\n",
    "2. **Separe cada string com delimitador ponto e vírgula** (`'A;B' -> ['A','B']`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sXGqnhCFxQ_V",
    "outputId": "96e7b954-fde6-4241-cecf-a03ddba682aa",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stl_rO4kb8rK"
   },
   "source": [
    "3. **Transforme as strings que contém números em floats** (`'2' -> 2`)\n",
    "* [pyspark.RDD.map](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html)\n",
    "* [string to float](https://stackoverflow.com/questions/379906/how-do-i-parse-a-string-to-a-float-or-int)\n",
    "\n",
    "  Convert str to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2pdJDbYAyAr_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSc0jg4ezOmY"
   },
   "source": [
    "## Tarefa 2: Analisando a lista de compras\n",
    "\n",
    "* [pyspark.RDD.map](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html)\n",
    "* [Como checar se um número é inteiro](https://stackoverflow.com/a/9266979)\n",
    "\n",
    "\n",
    "4. **Encontre a quantidade de itens comprados (se for um número quebrado arredonde para 1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibPTUpewzI53"
   },
   "source": [
    "5. **Obtenha o valor total da compra**\n",
    "\n",
    "\n",
    "* [pyspark.RDD.map](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BMMj9-ez0Vea"
   },
   "source": [
    "6. **Qual o produto mais caro, em termos de preço unitário?**\n",
    "\n",
    "\n",
    "* [pyspark.RDD.map](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.map.html)\n",
    "* [pyspark.RDD.takeOrdered](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.RDD.takeOrdered.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iEi5g7Yp6U0T",
    "outputId": "b00af403-b07f-46ba-b834-3b3e29e2f619"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
