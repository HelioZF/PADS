{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVBZIPfYJiVT"
   },
   "source": [
    "Insper\n",
    "\n",
    "# Aula 6 - DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:17:23.734268Z",
     "start_time": "2021-05-21T00:17:16.123247Z"
    },
    "id": "b2OJVWpSJiVf"
   },
   "outputs": [],
   "source": [
    "# Criar a sessao do Spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .appName(\"Michel_DF\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:17:26.088238Z",
     "start_time": "2021-05-21T00:17:26.064592Z"
    },
    "id": "X_uHmyPbJiVm"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-michel-20silva-20fornaciali:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0-preview2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Michel_DF</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f31408fb8c0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:17:27.899075Z",
     "start_time": "2021-05-21T00:17:27.891439Z"
    },
    "id": "DJONGIw8JiVp"
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5rPPVs9JiVq"
   },
   "source": [
    "Até agora vimos trabalhando com a estrutura base do Spark, as RDDs. Contudo, essas estruturas são trabalhosas e requerem o tratamento manual de tipos de dados bem como a interpretação constante dos dados posicionais na RDD.\n",
    "\n",
    "É possível simplificar nosso trabalho com o uso de camadas novas e mais elevadas do Spark.\n",
    "\n",
    "\n",
    "Existe uma série de funções para a criação e a criação e manipulação destas novas estruturas de dados. Vamos começar investigando os DataFrames, inspirados na biblioteca Pandas do Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSFf5T_7JiVs"
   },
   "source": [
    "## createDataFrame()\n",
    "Para criar um DataFrame a partir de uma RDD, podemos utilizar a função `createDataFrame(RDD)`. É importante notar que o ponto de entrada das APIs de dados estruturados não é mais o `sparkContext`, mas diretamente a `SparkSession` acessível a partir da nossa variável `spark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:20:53.995743Z",
     "start_time": "2021-05-21T00:20:53.960306Z"
    },
    "id": "f6wzQG_wJiVu"
   },
   "outputs": [],
   "source": [
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:20:59.480156Z",
     "start_time": "2021-05-21T00:20:59.344274Z"
    },
    "id": "OYIgjGJpJiVv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:21:19.780167Z",
     "start_time": "2021-05-21T00:21:18.710684Z"
    },
    "id": "EFaEkkXaJiVx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:21:22.105019Z",
     "start_time": "2021-05-21T00:21:22.095313Z"
    },
    "id": "NDYRgWy6JiVy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:22:41.521281Z",
     "start_time": "2021-05-21T00:22:41.510506Z"
    },
    "id": "sT_f_nPYJiVz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6u2_ExutJiV0"
   },
   "source": [
    "Verificamos que a variável é do tipo DataFrame com colunas com nome `_1, _2,` e diferentes tipos, já __inferidos__ pelo Spark na criação do DataFrame a partir da RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apT8qjmtJiV2"
   },
   "source": [
    "## take()\n",
    "Podemos então utilizar a função take para verificar nosso DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:23:11.797527Z",
     "start_time": "2021-05-21T00:23:11.583109Z"
    },
    "id": "ALatHfQLJiV4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dl_vUx7TJiV5"
   },
   "source": [
    "É interessante observar que a função nos mostra uma RDD e não um DataFrame. Essa RDD é composta de objetos do tipo Row, ou então linha. Isso é, um DataFrame nada mais é do que uma RDD onde cada elemento é uma linha de uma tabela com suas diferentes colunas. \n",
    "\n",
    "Diferentemente das RDDs padrões, contudo, os objetos Row grava consigo os nomes e os tipos das colunas e o Spark mantém a gestão garantindo que um mesmo DataFrame seja composto de linhas do mesmo tipo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6qXdyEUJiV8"
   },
   "source": [
    "## Row()\n",
    "\n",
    "Podemos manualmente criar uma linha, a partir da classe Row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:24:12.314546Z",
     "start_time": "2021-05-21T00:24:12.310669Z"
    },
    "id": "C9CLvHlOJiV9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klGT7PLMJiV9"
   },
   "source": [
    "Para isso precisamos nomear as colunas e seus valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:24:33.556002Z",
     "start_time": "2021-05-21T00:24:33.550280Z"
    },
    "id": "ErNt0M6WJiV-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvt_OY0kJiV_"
   },
   "source": [
    "Mais tarde entenderemos como concatenar diferentes objetos Row em um DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phDabM0bJiWA"
   },
   "source": [
    "## asDict()\n",
    "\n",
    "Podemos transformar objetos do tipo Row em dicionários no python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:25:04.122126Z",
     "start_time": "2021-05-21T00:25:04.020307Z"
    },
    "id": "vFzubGeLJiWB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0szOjGQsJiWC"
   },
   "source": [
    "Para isso, vamos inicialmente guardar duas linhas de nosso DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:25:20.921552Z",
     "start_time": "2021-05-21T00:25:20.820444Z"
    },
    "id": "Re124bTZJiWD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-TnIElGJiWD"
   },
   "source": [
    "Na sequencia utilizamos a função asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:25:45.766270Z",
     "start_time": "2021-05-21T00:25:45.760421Z"
    },
    "id": "n9Dnt2UQJiWE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zpd9rCfxJiWF"
   },
   "source": [
    "Com isso podemos acessar os valores de cada coluna normalmente como um dicionário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:26:23.845737Z",
     "start_time": "2021-05-21T00:26:23.840244Z"
    },
    "id": "g7HyXx5VJiWF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1B1Oe4q8JiWG"
   },
   "source": [
    "## RDD\n",
    "O atributo rdd do DataFrame também nos dá acesso direto à RDD fundamental que constrói o DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:26:43.660095Z",
     "start_time": "2021-05-21T00:26:43.630309Z"
    },
    "id": "JDUpa3ZZJiWH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxaC9-kxJiWH"
   },
   "source": [
    "Nessa RDD podemos aplicar todas as transformações e ações que aprendemos, desde que respeitando o tipo do elemento Row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:27:07.222509Z",
     "start_time": "2021-05-21T00:27:07.000299Z"
    },
    "id": "r3osxoEtJiWI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcitdRwlJiWJ"
   },
   "source": [
    "# Acessando o DataFrame\n",
    "A ideia de termos um DataFrame, contudo, é não precisarmos operar diretamente nas RDDs. Para isso existe um conjunto de funções de acesso direto ao DataFrame. Como estas funções operam intrinsecamente em RDDs, elas também são transformações e ações.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d164XEI9JiWK"
   },
   "source": [
    "## show()\n",
    "A ação show() nos permite olhar o DataFrame como uma tabela, muito mais próximo do Pandas DataFrame. Cuidado, contudo, com o desejo de olhar o DataFrame completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:27:44.225248Z",
     "start_time": "2021-05-21T00:27:44.041299Z"
    },
    "id": "F6LPgo4zJiWL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iuqddWyJiWL"
   },
   "source": [
    "## printSchema()\n",
    "\n",
    "A ação show() não nos mostra claramente qual é o tipo dos dados envolvidos (similar a dtypes). Para isso, podemos imprimir o Schema do DataFrame. \n",
    "\n",
    "O Schema é a definição do tipo de dado de cada coluna. É importante notar que o Spark inferiu esses dados a partir da RDD. O Spark infere a partir do primeiro elemento nos dados. Portanto, é importante que na leitura de arquivos, caso a inferência seja utilizada, garantir que a primeira linha do arquivo possua o tipo correto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:28:06.542170Z",
     "start_time": "2021-05-21T00:28:06.530567Z"
    },
    "id": "bPtzOWSiJiWM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W24MBB2lJiWN"
   },
   "source": [
    "## dtypes\n",
    "\n",
    "Podemos também utilizar o atributo dtypes para mostrar o tipo de cada coluna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:28:24.737418Z",
     "start_time": "2021-05-21T00:28:24.731344Z"
    },
    "id": "FcOetGJYJiWO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfJ9BdkAJiWO"
   },
   "source": [
    "# Definindo o Schema\n",
    "Idealmente, quando operando em grandes massas de dados, a inferência do tipo pela primeira linha não é adequada e utilizamos como boa prática a definição manual do schema. Para isso precisamos utilizar os tipos de dados nativos do Spark que são convertidos para e de tipos python pela API pyspark.\n",
    "\n",
    "Para isso importamos os tipos do spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:28:43.544586Z",
     "start_time": "2021-05-21T00:28:43.540393Z"
    },
    "id": "YfHDpfnUJiWP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLpeDCtyJiWQ"
   },
   "source": [
    "## StructType(), StructField(), IntegerType(), StringType(), FloatType()\n",
    "Cada tipo de variável é na verdade um objeto de uma classe. Então acessamos estes objetos para instanciar e para identificar os tipos de cada variável. \n",
    "\n",
    "O Schema de um DataFrame é definido como um objeto StructType (similar a uma lista ou tupla) com campos do tipo StructField(). Cada campo (StructField) possui um nome, um tipo (objeto do tipo no spark) e um flag se é permitida a existência de nulos. \n",
    "\n",
    "Assim, podemos definir explicitametne um Schema para nosso DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:31:14.074827Z",
     "start_time": "2021-05-21T00:31:14.070260Z"
    },
    "id": "Nz_YqnkFJiWR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZaZj0VMJiWR"
   },
   "source": [
    "Na sequencia, atribuímos o nosso schema ao DataFrame na sua criação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:31:14.959049Z",
     "start_time": "2021-05-21T00:31:14.920363Z"
    },
    "id": "c4WQVLM7JiWS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "011z0FrKJiWT"
   },
   "source": [
    "Podemos então olhar nosso DataFrame e verificar que as colunas agora vêm identificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:31:16.272213Z",
     "start_time": "2021-05-21T00:31:16.090423Z"
    },
    "id": "fGc42ofXJiWT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:31:27.848904Z",
     "start_time": "2021-05-21T00:31:27.843263Z"
    },
    "id": "Y4aMBaG3JiWU"
   },
   "source": [
    "Podemos também verificar o Schema do DataFrame com as características que definimos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:31:39.338187Z",
     "start_time": "2021-05-21T00:31:39.333233Z"
    },
    "id": "T_Fu_foZJiWV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npddbeDlJiWV"
   },
   "source": [
    "# Operações básicas com colunas\n",
    "\n",
    "A exemplo do Pandas DataFrame, temos à nossa disposição uma série de transformações e ações que nos permite operar sobre o Spark DataFrame. Veremos a seguir as operações em colunas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhpNDcVLJiWW"
   },
   "source": [
    "## columns\n",
    "O atributo columns retorna, a exemplo do Pandas, uma lista com o nome das colunas. Contudo, diferente do Pandas, não é possível sobrescrever este atributo diretamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:32:11.765697Z",
     "start_time": "2021-05-21T00:32:11.760307Z"
    },
    "id": "UT9n7-MQJiWX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clf8dytvJiWX"
   },
   "source": [
    "## withColumnRenamed()\n",
    "Para renomear uma coluna utilizamos a função withColumnRenamed(). Esta função opera em uma coluna por vez. Assim, podemos utilizar um laço para renomear todas as colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:32:42.550595Z",
     "start_time": "2021-05-21T00:32:42.531472Z"
    },
    "id": "FePzuZMQJiWY"
   },
   "outputs": [],
   "source": [
    "for old_col, new_col in zip(sdf.columns, ['dept_name', 'dept_id']):\n",
    "    sdf = sdf.withColumnRenamed(old_col, new_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:32:46.975020Z",
     "start_time": "2021-05-21T00:32:46.790250Z"
    },
    "id": "seWCGQciJiWZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-2NQ4U0JiWa"
   },
   "source": [
    "## drop()\n",
    "Podemos descartar colunas usando a função .drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:33:07.468100Z",
     "start_time": "2021-05-21T00:33:07.230097Z"
    },
    "id": "Dtqt1X3jJiWc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dG4j9r1uJiWc"
   },
   "source": [
    "## Acessando colunas\n",
    "Para acessar as colunas de um DataFrame, podemos utilizar a mesma notação python. Observe, contudo, que as colunas são representadas como objetos e, assim, o acesso à coluna não retorna automaticamente os dados existentes nela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:33:32.526381Z",
     "start_time": "2021-05-21T00:33:32.450232Z"
    },
    "id": "5YYiWqGuJiWd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:33:42.625523Z",
     "start_time": "2021-05-21T00:33:42.570266Z"
    },
    "id": "Nt16jBv-JiWe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgDh9C7XJiWe"
   },
   "source": [
    "## select()\n",
    "Para acessar os dados de uma coluna específica, precisamos utilizar a transformação select() seguida de uma ação show(). Esta transformação é similiar ao SELECT em SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:33:58.099007Z",
     "start_time": "2021-05-21T00:33:57.920370Z"
    },
    "id": "s9KV3cCSJiWf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VZH0IMnJiWg"
   },
   "source": [
    "## Case sensitivity\n",
    "Observe que a exemplo do SQL, DataFrames em Spark não são naturalmente sensíveis ao caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:34:15.501565Z",
     "start_time": "2021-05-21T00:34:15.320632Z"
    },
    "id": "bCDWUYJoJiWg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:34:34.631098Z",
     "start_time": "2021-05-21T00:34:34.440421Z"
    },
    "id": "PM0zteTYJiWh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfWrNMcWJiWi"
   },
   "source": [
    "# Operações básicas com linhas\n",
    "Da mesma forma que temos operações com colunas, temos operações com linhas.\n",
    "\n",
    "## limit()\n",
    "A transformação limit(LIM) nos permite limitar um número de registros (linhas no DataFrame) a ser retornado. POdemos concatenar com .collect() e temos o DataFrame no formato de RDD. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:35:13.374964Z",
     "start_time": "2021-05-21T00:35:13.200477Z"
    },
    "id": "Yn6WIpLtJiWi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dh8SZ_kEJiWj"
   },
   "source": [
    "Com isso podemos reduzir o número de linhas do nosso DataFrame para aumentar a velocidade do nosso trabalho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:35:36.000515Z",
     "start_time": "2021-05-21T00:35:35.990571Z"
    },
    "id": "MIJs_UuTJiWk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ntr6oazvJiWl"
   },
   "source": [
    "## filter() - filtrando linhas\n",
    "A transformação filter() nos permite filtrar linhas com base em condições especificadas sobre colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:36:02.834516Z",
     "start_time": "2021-05-21T00:36:02.453420Z"
    },
    "id": "_EiB-3Q1JiWm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3ccByLcJiWn"
   },
   "source": [
    "## where() - filtrando linhas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:36:17.736282Z",
     "start_time": "2021-05-21T00:36:17.730335Z"
    },
    "id": "7e9gZNOhJiWo"
   },
   "source": [
    "Para manter a similaridade como o SQL, a transformação filter() também pode ser acessada através de seu apelido (alias) where()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:36:36.380428Z",
     "start_time": "2021-05-21T00:36:36.170693Z"
    },
    "id": "XLX2n7KwJiWq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELnOoY6qJiWr"
   },
   "source": [
    "## distinct() - filtrando valores distintos\n",
    "Utilizamos a transformação distinct() para selecionar apenas os valores distintos de uma coluna específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:37:01.885896Z",
     "start_time": "2021-05-21T00:37:01.240361Z"
    },
    "id": "HUnW0ca8JiWu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upSmk_q8JiWv"
   },
   "source": [
    "## union() - concatenando linhas\n",
    "Para concatenar linhas no DataFrame, precisamos criar uma RDD com objetos do tipo Row e Schema idêntico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:38:28.674744Z",
     "start_time": "2021-05-21T00:38:28.670193Z"
    },
    "id": "ZurNo4QdJiWw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:38:34.775686Z",
     "start_time": "2021-05-21T00:38:34.770448Z"
    },
    "id": "poCgsFKRJiWx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyzhOqhZJiWy"
   },
   "source": [
    "Transformamos essa lista em uma RDD, usando parallelize(), da mesma forma que fizemos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:39:00.869358Z",
     "start_time": "2021-05-21T00:39:00.860407Z"
    },
    "id": "26zYJRg-JiWz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rbQwTPDJiW0"
   },
   "source": [
    "Por fim, criamos o DataFrame, definindo o Schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:39:28.912117Z",
     "start_time": "2021-05-21T00:39:28.870363Z"
    },
    "id": "Gg295LwDJiW2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:39:35.790342Z",
     "start_time": "2021-05-21T00:39:35.630421Z"
    },
    "id": "NJiZp7wbJiW3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfF00auYJiW3"
   },
   "source": [
    "Podemos então concatenar nosso novo DataFrame ao DataFrame antigo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:40:02.349584Z",
     "start_time": "2021-05-21T00:40:02.283006Z"
    },
    "id": "e-azi7fSJiW5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:40:11.145556Z",
     "start_time": "2021-05-21T00:40:10.701273Z"
    },
    "id": "NfdvzsKOJiW6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgLqF_2XJiW6"
   },
   "source": [
    "## orderBy() - ordenando linhas\n",
    "Com a transformação orderBy() podemos ordenar as linhas do DataFrame. Para isso, precisamos utilizar os métodos .asc() ou .desc() na coluna chave, indicando a ordem desejada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:40:48.398791Z",
     "start_time": "2021-05-21T00:40:48.000270Z"
    },
    "id": "kq4087edJiW8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFe0a2vPJiW9"
   },
   "source": [
    "# pyspark.sql.functions\n",
    "Existe uma série de funções que facilitam a manipulação de DataFrames. Elas estão disponíveis no pacote pyspark.sql.functions e podem ser investigadas na referência abaixo:\n",
    "\n",
    "http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#module-pyspark.sql.functions\n",
    "\n",
    "Podemos importá-las todas através da linha de comando abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:41:27.216458Z",
     "start_time": "2021-05-21T00:41:27.200290Z"
    },
    "id": "ajn3Fu4ZJiW-"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsXz_F4DJiW_"
   },
   "source": [
    "## lit() - criando colunas\n",
    "Como visto anteriormente, o Spark possui tipos próprios de dados com correspondência com o Python. Se quisermos criar uma coluna constante, podemos evitar o trabalho de criar a coluna em Python e transformá-la, utilizando a função lit(val) que cria uma coluna de literais (constantes) val."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:41:44.868609Z",
     "start_time": "2021-05-21T00:41:44.820337Z"
    },
    "id": "jB0N2-IsJiXB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuIFQ959JiXB"
   },
   "source": [
    "## withColumn() - adicionando colunas\n",
    "\n",
    "Para isso, precisamos concatenar uma coluna de literais. Isso é feito com a transformação withColumn(nome, nova_col)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:42:40.713566Z",
     "start_time": "2021-05-21T00:42:40.373309Z"
    },
    "id": "fo02aqO6JiXD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hoqj5LzJiXE"
   },
   "source": [
    "## col() - acessando colunas\n",
    "\n",
    "Em alguns casos precisamos explicitar que desejamos acessar um objeto do tipo Column. Para isso temos à nossa disposição a função .col(nome)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:42:44.644237Z",
     "start_time": "2021-05-21T00:42:44.250259Z"
    },
    "id": "4oKyEFg1JiXF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3tdTHvDJiXG"
   },
   "source": [
    "Observe que temos, com isso, diferentes formas de acessar colunas no select. Em alguns casos específicos será necessário optar pela menção explícita ao tipo do objeto usando .col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:43:16.281709Z",
     "start_time": "2021-05-21T00:43:15.967344Z"
    },
    "id": "Y75w2diGJiXG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPZcGGYaJiXH"
   },
   "source": [
    "Podemos ainda acessar a RDD resultante da transformação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:43:53.141443Z",
     "start_time": "2021-05-21T00:43:52.861402Z"
    },
    "id": "e5KQRhetJiXI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0nLbwS1JiXI"
   },
   "source": [
    "## alias()\n",
    "\n",
    "A exemplo do SQL, podemos utilizar o método .alias() de uma coluna específica para renomear colunas rapidamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:45:43.459353Z",
     "start_time": "2021-05-21T00:45:43.260528Z"
    },
    "id": "su8c3npDJiXJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YT0oBzTJiXK"
   },
   "source": [
    "# Expressões\n",
    "DataFrames são equivalentes de tabelas no Pandas e também de tabelas no SQL. Buscando manter compatibilidade com o SQL, expressões permitem que escrevamos algumas expressões simplificadas de SQL para operar no DataFrame.\n",
    "\n",
    "\n",
    "## expr()\n",
    "Para isso, utilizamos a função expr(). Podemos, por exemplo, renomear colunas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:54:44.670058Z",
     "start_time": "2021-05-21T00:54:43.897373Z"
    },
    "id": "6fnpe5maJiXL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rh5u383jJiXN"
   },
   "source": [
    "Podemos também operar sobre colunas diretamente, a exemplo do SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:55:25.186146Z",
     "start_time": "2021-05-21T00:55:24.900466Z"
    },
    "id": "lQjHYGv9JiXP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYrlbN7qJiXQ"
   },
   "source": [
    "## selectExpr()\n",
    "\n",
    "Como a combinação de .select() e .expr() é muito comum, o spark já nos fornece um atalho: selectExpr()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:56:23.388730Z",
     "start_time": "2021-05-21T00:56:23.180428Z"
    },
    "id": "Xl5bL80IJiXR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEcimQS7JiXS"
   },
   "source": [
    "# Abrindo arquivos CSV\n",
    "Até agora trabalhamos com DataFrames criados manualmente a partir de uma RDD prévia. Podemos utilizar as funções do pacote .read do Spark para ler diretamente fontes de dados. Para carregar arquivos .csv usamos a função .csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://files.grouplens.org/datasets/movielens/ml-25m.zip\n",
    "!unzip ml-25m.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T00:56:53.756026Z",
     "start_time": "2021-05-21T00:56:53.310648Z"
    },
    "id": "M0tznDXTJiXW"
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('ratings.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T19:53:47.626787Z",
     "start_time": "2021-05-20T19:53:47.565414Z"
    },
    "id": "HQpAU4oeJiXX"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T19:53:59.440442Z",
     "start_time": "2021-05-20T19:53:59.015732Z"
    },
    "id": "w5CHY8GkJiXY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T19:54:11.793463Z",
     "start_time": "2021-05-20T19:54:11.535305Z"
    },
    "id": "2OWdwstWJiXZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T19:55:02.741016Z",
     "start_time": "2021-05-20T19:55:02.735348Z"
    },
    "id": "OiGu4Fd4JiXa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T19:55:09.291075Z",
     "start_time": "2021-05-20T19:55:09.285145Z"
    },
    "id": "q6qBnXfPJiXb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T19:55:18.054126Z",
     "start_time": "2021-05-20T19:55:18.045350Z"
    },
    "id": "1GCzWjaCJiXb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T19:58:59.859416Z",
     "start_time": "2021-05-20T19:58:59.855595Z"
    },
    "id": "kSCZZcFeJiXc"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, DoubleType, TimestampType, IntegerType, StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:25:29.759760Z",
     "start_time": "2021-05-20T20:25:29.755337Z"
    },
    "id": "kWwdyX1MJiXf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:25:30.061298Z",
     "start_time": "2021-05-20T20:25:30.055250Z"
    },
    "id": "DUPrrtJbJiXg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:25:30.480166Z",
     "start_time": "2021-05-20T20:25:30.475509Z"
    },
    "id": "eieSrsVkJiXh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:25:31.466072Z",
     "start_time": "2021-05-20T20:25:31.435433Z"
    },
    "id": "eqisgLHpJiXi"
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('../10_dados/movie_lens/ratings.csv', header=True, schema = schema )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:25:31.921871Z",
     "start_time": "2021-05-20T20:25:31.916617Z"
    },
    "id": "2LK3Ey6cJiXj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:25:34.493677Z",
     "start_time": "2021-05-20T20:25:34.125235Z"
    },
    "id": "WO6bi5s2JiXl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:27:11.017949Z",
     "start_time": "2021-05-20T20:27:10.725449Z"
    },
    "id": "zuIywrgTJiXl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:27:57.440840Z",
     "start_time": "2021-05-20T20:27:57.028609Z"
    },
    "id": "DbpP-RXBJiXm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:29:20.465692Z",
     "start_time": "2021-05-20T20:29:20.445417Z"
    },
    "id": "2FmSGviOJiXn"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:29:48.461981Z",
     "start_time": "2021-05-20T20:29:48.443655Z"
    },
    "id": "ivMD_6DVJiXo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:29:49.315111Z",
     "start_time": "2021-05-20T20:29:49.035698Z"
    },
    "id": "4ot9EkXYJiXp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:31:25.436192Z",
     "start_time": "2021-05-20T20:31:25.385370Z"
    },
    "id": "QA0UK5QxJiXq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:31:30.884229Z",
     "start_time": "2021-05-20T20:31:30.475631Z"
    },
    "id": "18nvZwSdJiXr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:32:08.470291Z",
     "start_time": "2021-05-20T20:32:08.455277Z"
    },
    "id": "qfXOI2eNJiXt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:32:11.036945Z",
     "start_time": "2021-05-20T20:32:10.878419Z"
    },
    "id": "dSV_Yb_XJiXv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:33:18.929177Z",
     "start_time": "2021-05-20T20:32:49.955959Z"
    },
    "id": "3FhqV3JwJiXw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:33:56.640954Z",
     "start_time": "2021-05-20T20:33:49.385757Z"
    },
    "id": "s8fye18SJiXx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:35:09.794484Z",
     "start_time": "2021-05-20T20:35:09.505251Z"
    },
    "id": "TAAuCn-TJiXy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:36:18.533696Z",
     "start_time": "2021-05-20T20:36:18.515114Z"
    },
    "id": "3LsLzyG2JiXy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:37:56.319942Z",
     "start_time": "2021-05-20T20:37:56.295157Z"
    },
    "id": "Lf_9Vq1ZJiXz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:38:39.410096Z",
     "start_time": "2021-05-20T20:38:39.098228Z"
    },
    "id": "YRFPBb-eJiX0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:42:00.169356Z",
     "start_time": "2021-05-20T20:42:00.055291Z"
    },
    "id": "x2dvGTjEJiX1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:42:01.808240Z",
     "start_time": "2021-05-20T20:42:01.700936Z"
    },
    "id": "BEwaqTk3JiX2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:42:28.042780Z",
     "start_time": "2021-05-20T20:42:04.635241Z"
    },
    "id": "zFOl4qPLJiX3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:43:42.209948Z",
     "start_time": "2021-05-20T20:43:42.205681Z"
    },
    "id": "g9Jv8xGLJiX5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:45:54.751926Z",
     "start_time": "2021-05-20T20:45:29.688390Z"
    },
    "id": "wCRYAF6uJiX6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:46:00.514313Z",
     "start_time": "2021-05-20T20:46:00.395710Z"
    },
    "id": "Ux-Ow067JiX7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:46:51.203492Z",
     "start_time": "2021-05-20T20:46:51.179137Z"
    },
    "id": "MdQsXQKgJiX8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:49:52.961714Z",
     "start_time": "2021-05-20T20:49:28.855213Z"
    },
    "id": "jpMYaSnGJiX8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T20:48:45.231492Z",
     "start_time": "2021-05-20T20:48:21.805178Z"
    },
    "id": "uD0UYDJSJiX9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9g2ffclDJiYA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "nteract": {
   "version": "0.28.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
